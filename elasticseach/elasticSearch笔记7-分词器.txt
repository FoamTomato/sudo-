**分词器介绍及内置分词器**

分词器：从一串文本中切分出一个一个的词条，并对每个词条进行标准化

包括三部分：

character filter:分词之前的预处理，过滤掉HTML标签，特殊符号转换等

tokenizer:分词

token filter:标准化

**内置分词器**

standard分词器：（默认的）他会将词汇单元转换成小写形式，并去除停用词和标点符号，支持中文采用的方法为单字切分

simple分词器：首先会通过非字母字符来分割文本信息，然后将词汇单元统一为小写形式，该分词器会去掉数字类型的字符

Whitespace分词器：仅仅是去除空格，对字符没有lowcase化，不支持中文；并且不对生成的词汇单元进行其他的标准化处理

language分词器：特定语言的分词器，不支持中文

### 安装中文分词器

1.下载中文分词器 https://github.com/medcl/elasticsearch-analysis-ik

```
下载elasticsearch-analysis-ik-master.zip
```

2.解压

unzip elasticsearch-analysis-ik-master.zip

3.进入elasticsearch-analysis-ik-master,编译源码

mvn clean install -Dmaven.test.skip=true

4.在es的plugins文件夹下创建目录ik

/Foam/elasticsearch-analysis-ik-master/target/releases

5.将编译后生成的elasticsearch-analysis-ik-版本.zip移动到ik下，并解压

6.解压后的内容移动到ik目录下